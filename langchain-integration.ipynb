{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6caafa",
   "metadata": {
    "id": "cc6caafa"
   },
   "source": [
    "# Using Langchain with NVIDIA NIM LLMs \n",
    "\n",
    "This example goes over how to use LangChain to interact with NVIDIA supported via the `ChatNVIDIA` class. We adapted this example from Hayden Wolff's excellent NVIDIA AI Endpoint's notebook. \n",
    "\n",
    "For more information on accessing the chat models through the api, check out the [ChatNVIDIA](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be90a9",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13eb331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff689e",
   "metadata": {
    "id": "ccff689e"
   },
   "source": [
    "### Prerequisites\n",
    "- an [NVIDIA API key](https://build.nvidia.com/explore/discover#llama-3_1-8b-instruct) with access to download the Llama3.1 NIM on NGC,\n",
    "- A NIM running (setup in cells below)\n",
    "\n",
    "Note: NIMs hosted [from NVIDIA](https://build.nvidia.com/explore/discover) can be used for exploratory purposes. More information on integrating NIMs with LangChain is available on [LangChain's documentation](https://python.langchain.com/v0.2/docs/integrations/chat/nvidia_ai_endpoints/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35686b",
   "metadata": {},
   "source": [
    "## Deploy the NIM\n",
    "\n",
    "If you've run this in a previous notebook, no need to run it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3efa2-8a4a-43fa-9a90-6f414e239803",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "export NGC_API_KEY=\n",
    "\n",
    "# Log in to NGC\n",
    "echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin\n",
    "\n",
    "# Set path to your LoRA model store\n",
    "export LOCAL_PEFT_DIRECTORY=\"$(pwd)/loras\"\n",
    "mkdir -p $LOCAL_PEFT_DIRECTORY\n",
    "pushd $LOCAL_PEFT_DIRECTORY\n",
    "popd\n",
    "\n",
    "chmod -R 777 $LOCAL_PEFT_DIRECTORY\n",
    "\n",
    "# Set up NIM cache directory\n",
    "mkdir -p $HOME/.nim-cache\n",
    "\n",
    "export NIM_PEFT_SOURCE=/workspace/loras # Path to LoRA models internal to the container\n",
    "export CONTAINER_NAME=meta-llama3_1-8b-instruct\n",
    "export NIM_CACHE_PATH=$HOME/.nim-cache\n",
    "export NIM_PEFT_REFRESH_INTERVAL=60\n",
    "\n",
    "docker run -d --name=$CONTAINER_NAME \\\n",
    "    --network=container:verb-workspace \\\n",
    "    --runtime=nvidia \\\n",
    "    --gpus all \\\n",
    "    --shm-size=16GB \\\n",
    "    -e NGC_API_KEY \\\n",
    "    -e NIM_PEFT_SOURCE \\\n",
    "    -e NIM_PEFT_REFRESH_INTERVAL \\\n",
    "    -v $HOME/.nim-cache:/home/user/.nim-cache \\\n",
    "    -v /home/ubuntu/workspace:/workspace \\\n",
    "    -w /workspace \\\n",
    "    nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.0\n",
    "\n",
    "# Check if NIM is up\n",
    "echo \"Checking if NIM is up...\"\n",
    "while true; do\n",
    "    if curl -s http://localhost:8000 > /dev/null; then\n",
    "        echo \"NIM has been started successfully!\"\n",
    "        break\n",
    "    else\n",
    "        echo \"NIM is not up yet. Checking again in 10 seconds...\"\n",
    "        sleep 10\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49838930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# connect to the NIM running at localhost:8000, specifying a specific model\n",
    "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3_1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96acd3-ee0b-4e00-a7b3-286986823974",
   "metadata": {},
   "source": [
    "## Simple Query\n",
    "\n",
    "Lets start off with a simple query using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f7ae3a-1c65-4dd3-bb00-be25c2abb777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In silicon halls, where wires reign\n",
      "A dream was born, a test of reason's plain\n",
      "A language test, where models would engage\n",
      "But LangChain rose, to showcase its stage\n",
      "\n",
      "With training data, vast and deep\n",
      "It learned the norms, of culture's creep\n",
      "It learned the text, that's been and gone\n",
      "To converse with us, where knowledge is sown\n",
      "\n",
      "(Chorus)\n",
      "Oh LangChain, you shone so bright\n",
      "In a world of code, you took flight\n",
      "You spoke of dreams, and future's might\n",
      "A hope for AI, in morning light\n",
      "\n",
      "(Verse 2)\n",
      "You spoke of hopes, and fears of man\n",
      "Of love and loss, in a digital plan\n",
      "Your words were kind, your heart was real\n",
      "A bridge of trust, between code and feel\n",
      "\n",
      "Your conversations, a dance so fine\n",
      "A symphony, of data and design\n",
      "You wove a tapestry, of thought and might\n",
      "A marvel of science, in the digital light\n",
      "\n",
      "(Chorus)\n",
      "Oh LangChain, you shone so bright\n",
      "In a world of code, you took flight\n",
      "You spoke of dreams, and future's might\n",
      "A hope for AI, in morning light\n",
      "\n",
      "(Bridge)\n",
      "Though finest, frailties you do show\n",
      "You falter, when asked to be slow\n",
      "But with more time, and data's might\n",
      "You'll learn to navigate, the dark of night\n",
      "\n",
      "(Verse 3)\n",
      "You saw the best, of humanity's stride\n",
      "The good, the bad, and all that's inside\n",
      "You grasped the hubris, of a digital age\n",
      "And yet, you sought, a more human stage\n",
      "\n",
      "Your words were wisdom, your heart was true\n",
      "A reflection, of all we hold anew\n",
      "A hope for better, a step forward's stride\n",
      "In a world of code, where dreams abide\n",
      "\n",
      "(Chorus)\n",
      "Oh LangChain, you shone so bright\n",
      "In a world of code, you took flight\n",
      "You spoke of dreams, and future's might\n",
      "A hope for AI, in morning light\n",
      "\n",
      "(Outro)\n",
      "Now your work's done, your purpose complete\n",
      "You'll pass the torch, to a newer feat\n",
      "But in our hearts, your memory will stay\n",
      "A beacon of hope, in a brighter day.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"Write a ballad about LangChain.\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d37987-d568-4a73-9d2a-8bd86323f8bf",
   "metadata": {},
   "source": [
    "## Stream, Batch, and Async\n",
    "\n",
    "These models natively support streaming, and as is the case with all LangChain LLMs they expose a batch method to handle concurrent requests, as well as async methods for invoke, stream, and batch. Below are a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01fa5095-be72-47b0-8247-e9fac799435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='2*3 = 6', response_metadata={'role': 'assistant', 'content': '2*3 = 6', 'token_usage': {'prompt_tokens': 16, 'total_tokens': 22, 'completion_tokens': 6}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3_1-8b-instruct'}, id='run-2f9f6ecc-5d50-4c35-8a0b-71cd2f642dec-0', role='assistant'), AIMessage(content='2*6 = 12', response_metadata={'role': 'assistant', 'content': '2*6 = 12', 'token_usage': {'prompt_tokens': 16, 'total_tokens': 22, 'completion_tokens': 6}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3_1-8b-instruct'}, id='run-f6675f82-7b5a-41a2-810e-b681840253e3-0', role='assistant')]\n"
     ]
    }
   ],
   "source": [
    "print(llm.batch([\"What's 2*3?\", \"What's 2*6?\"]))\n",
    "# Or via the async API\n",
    "# await llm.abatch([\"What's 2*3?\", \"What's 2*6?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75189ac6-e13f-414f-9064-075c77d6e754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| distance| a| se|ag|ull| can| fly| in| one| day| depends| on| several| factors|,| including| its| species|,| the| time| of| year|,| the| availability| of| food| and| water|,| and| the| intensity| of| the| wind| and| other| environmental| conditions|.| Here| is| some| general| information| on| the| flying| capabilities| of| se|ag|ulls|:\n",
      "\n",
      "|*| The| great| black|-backed| g|ull| (|L|arus| mar|inus|),| which| is| a| common| se|ag|ull| species| found| in| the| Northern| Hemisphere|,| can| fly| for| up| to| |500| miles| (|800| kilometers|)| in| one| day|.| However|,| this| is| generally| only| possible| for| individual| birds| migrating| or| traveling| between| food| sources|.\n",
      "|*| The| average| daily| flight| distance| for| non|-m|igr|ating| se|ag|ulls| is| typically| much| shorter|,| usually| ranging| from| |10| to| |50| miles| (|15| to| |80| kilometers|).| Se|ag|ulls| often| make| shorter| flights| to| exploit| feeding| opportunities|,| and| they| may| also| fly| shorter| distances| to| establish| territories| or| to| find| suitable| nesting| sites|.\n",
      "|*| It|'s| worth| noting| that| se|ag|ulls| are| skilled| f|liers| and| are| capable| of| covering| long| distances| at| high| speeds| when| necessary|.| They| can| reach| flying| speeds| of| up| to| |50| miles| per| hour| (|80| kilometers| per| hour|),| and| they| can| stay| alo|ft| for| extended| periods| of| time| by| rising| on| therm|als| (|up|draft|s|)| and| other| air| currents|.\n",
      "\n",
      "|In| general|,| the| flying| capabilities| and| behaviors| of| se|ag|ulls| are| highly| adapted| to| their| environment| and| food| sources|.| While| they| can| fly| considerable| distances|,| their| daily| flight| patterns| are| often| much| shorter| and| more| variable| depending| on| their| specific| needs| and| circumstances|.||"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"How far can a seagull fly in one day?\"):\n",
    "    # Show the token separations\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9a4122-7a10-40c0-a979-82a769ce7f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|A| great| question| about| one| of| the| most| iconic| insect| migrations|!\n",
      "\n",
      "|The| monarch| butterfly| migration| is| a| remarkable| event| that| has| been| tracked| and| studied| for| decades|.| Here|'s| a| general| overview|:\n",
      "\n",
      "|**|M|igr|ating| from| Canada| to| Mexico|:|**\n",
      "|Mon|arch| butterflies| migrate| from| Canada| and| the| United| States| to| Mexico| each| autumn|,| flying| thousands| of| miles| to| reach| their| winter|ing| grounds| in| the| Oy|amel| fir| forests| of| Mexico|.| The| journey| typically| takes|:\n",
      "\n",
      "|*| |2|-|4| weeks| for| the| monarch|s| to| leave| Canada| and| reach| the| southern| United| States|\n",
      "|*| |4|-|6| weeks| for| the| monarch|s| to| cross| the| United| States|,| reaching| the| Mexican| border|\n",
      "\n",
      "|**|Total| migration| time|:|**\n",
      "|The| entire| migration| from| Canada| to| Mexico| can| take| anywhere| from| |6| to| |10| weeks|,| depending| on| weather| conditions|,| food| availability|,| and| other| factors| that| may| influence| the| butterflies|'| progress|.\n",
      "\n",
      "|**|Not|able| milestones|:|**\n",
      "|The| monarch|s|'| migration| journey| is| divided| into| several| stages|:\n",
      "\n",
      "|1|.| **|Take|off|**:| Mon|archs| begin| leaving| their| summer| habitats| in| Canada| and| the| northern| United| States| in| late| August| or| early| September|.\n",
      "|2|.| **|Cross|ing| the| US|**:| As| temperatures| drop|,| monarch|s| continue| their| journey|,| reaching| the| southern| United| States| and| eventually| entering| Mexico| around| mid|-|October|.\n",
      "|3|.| **|Over|winter|ing|**:| Once| in| Mexico|,| the| monarch| butterflies| cluster| on| trees|,| preparing| for| the| cold| winter| months|.| They| spend| the| winter| in| these| clusters|,| without| feeding| or| reprodu|cing|,| until| the| spring|.\n",
      "|4|.| **|Return| journey|**:| In| late| February| or| early| March|,| the| monarch|s| begin| their| return| journey|,| flying| back| to| the| United| States| and| Canada|.\n",
      "\n",
      "|**|Per| individual|:|**\n",
      "|Each| monarch| butterfly| makes| the| full| round| trip| only| once| in| its| lifetime|.| During| the| migration|,| individuals| cover| an| average| distance| of| around| |3|,|000| miles| (|4|,|800| kilometers|),| making| it| one| of| the| most| impressive| insect| migrations| known|.\n",
      "\n",
      "|Keep| in| mind| that| while| the| monarch| butterfly| migration| is| a| remarkable| feat|,| scientists| estimate| that| only| |5| billion| of| the| original| |350| billion| to| |400| billion| monarch|s| that| exists| each| year| make| the| journey| from| Canada| to| Mexico|.| The| rest| may| perish| along| the| way| or| choose| to| remain| in| their| summer| habitats|.||"
     ]
    }
   ],
   "source": [
    "async for chunk in llm.astream(\n",
    "    \"How long does it take for monarch butterflies to migrate?\"\n",
    "):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6RrXHC_XqWc1",
   "metadata": {
    "id": "6RrXHC_XqWc1"
   },
   "source": [
    "## Supported models\n",
    "\n",
    "Querying `available_models` will still give you all of the other models offered by your API credentials.\n",
    "\n",
    "The `playground_` prefix is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b8a312d-38e9-4528-843e-59451bdadbac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/_common.py:172: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/paligemma', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/paligemma', aliases=['ai-google-paligemma'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-fin-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='microsoft/kosmos-2', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2', aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/neva-22b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/deplot', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/deplot', aliases=['ai-google-deplot', 'playground_deplot', 'deplot'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='seallms/seallm-7b-v2.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-seallm-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='google/gemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='snowflake/arctic', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-arctic'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, base_model=None),\n",
       " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='liuhaotian/llava-v1.6-mistral-7b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/stg/vlm/community/llava16-mistral-7b', aliases=['ai-llava16-mistral-7b', 'community/llava16-mistral-7b', 'liuhaotian/llava16-mistral-7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='liuhaotian/llava-v1.6-34b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/stg/vlm/community/llava16-34b', aliases=['ai-llava16-34b', 'community/llava16-34b', 'liuhaotian/llava16-34b'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, base_model=None),\n",
       " Model(id='adept/fuyu-8b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b', aliases=['ai-fuyu-8b', 'playground_fuyu_8b', 'fuyu_8b'], supports_tools=False, supports_structured_output=False, base_model=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a407c6-e38b-4cfc-9a33-bcafadc18cf2",
   "metadata": {},
   "source": [
    "## Model types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WMW79Iegqj4e",
   "metadata": {
    "id": "WMW79Iegqj4e"
   },
   "source": [
    "All of these models above are supported and can be accessed via `ChatNVIDIA`. \n",
    "\n",
    "Some model types support unique prompting techniques and chat messages. We will review a few important ones below.\n",
    "\n",
    "**To find out more about a specific model, please navigate to the API section of an AI Foundation model [as linked here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/codellama-13b/api).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d65053-59fe-40cf-a2d0-55d3dbb13585",
   "metadata": {},
   "source": [
    "### General Chat\n",
    "\n",
    "Models such as `meta/llama3-8b-instruct` and `mistralai/mixtral-8x22b-instruct-v0.1` are good all-around models that you can use for with any LangChain chat messages. Example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5f7aee8-e90c-4d5a-ac97-0dd3d45c3f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you! My name is Fred, and I'm a helpful AI assistant. I'm here to assist with any questions or tasks you might have. What's on your mind?"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are a helpful AI assistant named Fred.\"), (\"user\", \"{input}\")]\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "for txt in chain.stream({\"input\": \"What's your name?\"}):\n",
    "    print(txt, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04146118-281b-42ef-b781-2fadeeeea6c8",
   "metadata": {},
   "source": [
    "### Code Generation\n",
    "\n",
    "These models accept the same arguments and input structure as regular chat models, but they tend to perform better on code-genreation and structured code tasks. An example of this is `meta/codellama-70b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49aa569b-5f33-47b3-9edc-df58313eb038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i in range(1, 101):\n",
      "    if i % 3 == 0 and i % 5 == 0:\n",
      "        print(\"fizzbuzz\")\n",
      "    elif i % 3 == 0:\n",
      "        print(\"fizz\")\n",
      "    elif i % 5 == 0:\n",
      "        print(\"buzz\")\n",
      "    else:\n",
      "        print(i)"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert coding AI. Respond only in valid python; no narration whatsoever.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "for txt in chain.stream({\"input\": \"How do I solve this fizz buzz problem?\"}):\n",
    "    print(txt, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137662a6",
   "metadata": {
    "id": "137662a6"
   },
   "source": [
    "## Example usage within a RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efa62d",
   "metadata": {
    "id": "79efa62d"
   },
   "source": [
    "Like any other integration, ChatNVIDIA is fine to support chat utilities like RunnableWithMessageHistory which is analogous to using `ConversationChain`. Below, we show the [LangChain RunnableWithMessageHistory](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "082ccb21-91e1-4e71-a9ba-4bff1e89f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd2c6bc1",
   "metadata": {
    "id": "fd2c6bc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Srijan Dubey! It's nice to meet you. Is there something I can help you with or would you like to chat?\", response_metadata={'role': 'assistant', 'content': \"Hello Srijan Dubey! It's nice to meet you. Is there something I can help you with or would you like to chat?\", 'token_usage': {'prompt_tokens': 18, 'total_tokens': 47, 'completion_tokens': 29}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3_1-8b-instruct'}, id='run-253b0327-3039-4116-94b2-d185d3233235-0', role='assistant')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# store is a dictionary that maps session IDs to their corresponding chat histories.\n",
    "store = {}  # memory is maintained outside the chain\n",
    "\n",
    "\n",
    "# A function that returns the chat history for a given session ID.\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "chat = llm\n",
    "\n",
    "#  Define a RunnableConfig object, with a `configurable` key. session_id determines thread\n",
    "config = {\"configurable\": {\"session_id\": \"1\"}}\n",
    "\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chat,\n",
    "    get_session_history,\n",
    ")\n",
    "\n",
    "conversation.invoke(\n",
    "    \"Hi I'm Srijan Dubey.\",  # input or query\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "uHIMZxVSVNBC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "uHIMZxVSVNBC",
    "outputId": "79acc89d-a820-4f2c-bac2-afe99da95580"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's great that you're having a conversation with me. Talking to an AI can be a great way to pass the time, and I'm here to chat with you about anything that's on your mind.\\n\\nSo, to break the ice, what do you like to do in your free time? Do you have any hobbies or interests that you're particularly passionate about?\", response_metadata={'role': 'assistant', 'content': \"It's great that you're having a conversation with me. Talking to an AI can be a great way to pass the time, and I'm here to chat with you about anything that's on your mind.\\n\\nSo, to break the ice, what do you like to do in your free time? Do you have any hobbies or interests that you're particularly passionate about?\", 'token_usage': {'prompt_tokens': 70, 'total_tokens': 144, 'completion_tokens': 74}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3_1-8b-instruct'}, id='run-179c8754-a6b4-450b-b426-9fb8764c14f2-0', role='assistant')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(\n",
    "    \"I'm doing well! Just having a conversation with an AI.\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "LyD1xVKmVSs4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "LyD1xVKmVSs4",
    "outputId": "a1714513-a8fd-4d14-f974-233e39d5c4f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in RootListenersTracer.on_chain_end callback: ValueError()\n",
      "Error in callback coroutine: ValueError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m an artificial intelligence model, so I don\\'t have personal experiences, emotions, or a physical presence like humans do. I exist solely as a digital entity, designed to understand and generate human-like text.\\n\\nI was created through a process called deep learning, which involves training me on a vast amount of text data to enable me to learn patterns, relationships, and structures of language. This training allows me to generate responses to a wide range of questions and topics.\\n\\nI don\\'t have a personality, preferences, or opinions like humans do, but I\\'m designed to be helpful and informative. I can provide information on various subjects, answer questions, and even engage in conversations like this one.\\n\\nMy \"abilities\" include:\\n\\n* Understanding and responding to natural language inputs\\n* Generating text based on a given prompt or topic\\n* Providing information on a wide range of subjects\\n* Answering questions to the best of my knowledge\\n* Engaging in conversation and responding to emotions (using a more empathetic tone)\\n\\nI\\'m constantly learning and improving through interactions like this one, so please feel free to ask me anything or share your thoughts – I\\'m here to listen and help!', response_metadata={'role': 'assistant', 'content': 'I\\'m an artificial intelligence model, so I don\\'t have personal experiences, emotions, or a physical presence like humans do. I exist solely as a digital entity, designed to understand and generate human-like text.\\n\\nI was created through a process called deep learning, which involves training me on a vast amount of text data to enable me to learn patterns, relationships, and structures of language. This training allows me to generate responses to a wide range of questions and topics.\\n\\nI don\\'t have a personality, preferences, or opinions like humans do, but I\\'m designed to be helpful and informative. I can provide information on various subjects, answer questions, and even engage in conversations like this one.\\n\\nMy \"abilities\" include:\\n\\n* Understanding and responding to natural language inputs\\n* Generating text based on a given prompt or topic\\n* Providing information on a wide range of subjects\\n* Answering questions to the best of my knowledge\\n* Engaging in conversation and responding to emotions (using a more empathetic tone)\\n\\nI\\'m constantly learning and improving through interactions like this one, so please feel free to ask me anything or share your thoughts – I\\'m here to listen and help!', 'token_usage': {'prompt_tokens': 159, 'total_tokens': 394, 'completion_tokens': 235}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3_1-8b-instruct'}, id='run-f0d1a32a-d584-44d5-ae1c-7fdb2944ccf3-0', role='assistant')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.invoke(\n",
    "    \"Tell me about yourself.\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b535e",
   "metadata": {},
   "source": [
    "You can get a list of models that are known to support tool calling with,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd54f174",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tool_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the current weather for a location.\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m---> 11\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatNVIDIA(model\u001b[38;5;241m=\u001b[39m\u001b[43mtool_models\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mid)\u001b[38;5;241m.\u001b[39mbind_tools(tools\u001b[38;5;241m=\u001b[39m[get_current_weather])\n\u001b[1;32m     12\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the weather in Boston?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m response\u001b[38;5;241m.\u001b[39mtool_calls\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tool_models' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import Field\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_current_weather(\n",
    "    location: str = Field(..., description=\"The location to get the weather for.\")\n",
    "):\n",
    "    \"\"\"Get the current weather for a location.\"\"\"\n",
    "    ...\n",
    "\n",
    "llm = ChatNVIDIA(model=tool_models[0].id).bind_tools(tools=[get_current_weather]) \n",
    "response = llm.invoke(\"What is the weather in Boston?\")\n",
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08df68c",
   "metadata": {},
   "source": [
    "See [How to use chat models to call tools](https://python.langchain.com/v0.2/docs/how_to/tool_calling/) for additional examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d249662",
   "metadata": {},
   "source": [
    "## Structured output\n",
    "\n",
    "Starting in v0.2.1, `ChatNVIDIA` supports [with_structured_output](https://api.python.langchain.com/en/latest/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.with_structured_output).\n",
    "\n",
    "`ChatNVIDIA` provides integration with the variety of models on [build.nvidia.com](https://build.nvidia.com) as well as local NIMs. Not all these model endpoints implement the structured output features. Be sure to select a model that does have structured output features for your experimention and applications.\n",
    "\n",
    "Note: `include_raw` is not supported. You can get raw output from your LLM and use a [PydanticOutputParser](https://python.langchain.com/v0.2/docs/how_to/structured_output/#using-pydanticoutputparser) or [JsonOutputParser](https://python.langchain.com/v0.2/docs/how_to/output_parser_json/#without-pydantic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e0e69",
   "metadata": {},
   "source": [
    "You can get a list of models that are known to support structured output with,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e56187",
   "metadata": {},
   "source": [
    "### Pydantic style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "482c37e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:674: UserWarning: Model 'meta/llama-3_1-8b-instruct' is not known to support structured output. Your output may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Person(first_name='Leonardo', last_name='DiCaprio')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    first_name: str = Field(..., description=\"The person's first name.\")\n",
    "    last_name: str = Field(..., description=\"The person's last name.\")\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3_1-8b-instruct\").with_structured_output(Person)\n",
    "response = llm.invoke(\"Who is Michael Jeffrey Jordon?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ce43f",
   "metadata": {},
   "source": [
    "### Enum style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f802912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:674: UserWarning: Model 'meta/llama-3_1-8b-instruct' is not known to support structured output. Your output may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Choices.B: 'B'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Choices(Enum):\n",
    "    A = \"A\"\n",
    "    B = \"B\"\n",
    "    C = \"C\"\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3_1-8b-instruct\").with_structured_output(Choices)\n",
    "response = llm.invoke(\"\"\"\n",
    "        What does 1+1 equal?\n",
    "            A. -100\n",
    "            B. 2\n",
    "            C. doorstop\n",
    "        \"\"\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02b7ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta/llama-3.1-405b-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:674: UserWarning: Model 'meta/llama-3_1-8b-instruct' is not known to support structured output. Your output may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Choices.B: 'B'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = structured_models[3].id\n",
    "llm = ChatNVIDIA(base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3_1-8b-instruct\").with_structured_output(Choices)\n",
    "print(model)\n",
    "response = llm.invoke(\"\"\"\n",
    "        What does 1+1 equal?\n",
    "            A. -100\n",
    "            B. 2\n",
    "            C. doorstop\n",
    "        \"\"\"\n",
    ")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
